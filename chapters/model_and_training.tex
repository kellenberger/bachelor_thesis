\section{Components}

This section aims to give a short overview of the main techniques and architectures used in this thesis. Different types of recurrent neural networks are discussed as well as the sequence-to-sequence model and the attention mechanism. All these architectures are essential parts of the model used in this thesis.

\subsection{Types of Recurrent Neural Networks}
\label{rnn_types}

\begin{figure}[p]
\centering
\includegraphics[width=0.9\linewidth]{rnn}
\caption{Architecture of a vanilla recurrent neural network cell. Each timestep some output and a hidden state vector \(\mathbf{h}_t\) are produced by looking at the hidden state vector from the previous timestep \(\mathbf{h}_{t-1}\) and the current input \(\mathbf{x}_t\). In this simple example, \(\mathbf{h}_t\) is also the output.}
\label{rnn}
\end{figure}

A recurrent neural network (RNN)\cite{rnn} is a special form of neural network that is used for sequential tasks. It works by having multiple copies of the network, one for each timestep. As the input proceeds in time, the network passes information to its next instance as seen in Figure \ref{rnn}. For an input sequence \((\mathbf{x}_1, ..., \mathbf{x}_n)\) a very simple RNN produces at each timestep \(t\) a hidden state vector \(\mathbf{h}_t\) as follows:

\begin{equation*}
  \mathbf{h}_t = \tanh \left(\mathbf{W} \begin{pmatrix} \mathbf{x}_t \\ \mathbf{h}_{t-1} \end{pmatrix} \right)
\end{equation*}

\begin{figure}[p]
\centering
\includegraphics[width=0.9\linewidth]{lstm}
\caption{Architecture of a typical LSTM cell. In addition to the hidden state vector \(\mathbf{h}_t\), a memory state vector \(\mathbf{c}_t\) is passed to the next timestep. \(\mathbf{h}_{t-1}\) and the input \(\mathbf{x}_t\) are used to compute the gates \(\mathbf{f}_t\), \(\mathbf{i}_t\) and \(\mathbf{o}_t\) and the candidate vector \(\mathbf{g}_t\). The gates are then used to add, delete and retrieve information to respectively from \(\mathbf{c}_{t-1}\), subsequently generating \(\mathbf{c}_t\) and \(\mathbf{h}_t\).}
\label{lstm}
\end{figure}

However, vanilla RNNs have proven to be hard to train and to struggle with long-range dependencies \cite{hochreiter_rnn}. In theory, they should be able to deal with these dependencies but either vanishing or exploding gradients usually prevent them from doing so. These issues were addressed with the introduction of Long Short-Term Memory networks (LSTMs) \cite{lstm}. In addition to \(\mathbf{h}_t\), LSTMs also pass a memory state vector \(\mathbf{c}_t\) to the next instance as can be seen in Figure \ref{lstm}. The LSTM can choose at each timestep which information it wants to read or forget from the memory vector or which new one it wants to write onto the vector. This is done by using explicit gating mechanisms:

\begin{align*}
  \mathbf{f}_t &= \sigma \left(\mathbf{W}_f \begin{pmatrix} \mathbf{x}_t \\ \mathbf{h}_{t-1} \end{pmatrix} \right) &
  \mathbf{i}_t &= \sigma \left(\mathbf{W}_i \begin{pmatrix} \mathbf{x}_t \\ \mathbf{h}_{t-1} \end{pmatrix} \right) \\
  \mathbf{o}_t &= \sigma \left(\mathbf{W}_o \begin{pmatrix} \mathbf{x}_t \\ \mathbf{h}_{t-1} \end{pmatrix} \right) &
  \mathbf{g}_t &= \tanh \left(\mathbf{W}_g \begin{pmatrix} \mathbf{x}_t \\ \mathbf{h}_{t-1} \end{pmatrix} \right)
\end{align*}

Here \(\sigma\) is the sigmoid function. \(\mathbf{f}_t\), \(\mathbf{i}_t\) and \(\mathbf{o}_t\) can be thought of as binary gates that decide which information from \(\mathbf{c}_{t-1}\) should be deleted, which information of \(\mathbf{c}_{t-1}\) should be updated and which information from \(\mathbf{c}_t\) should be written to \(\mathbf{h}_t\). Finally, \(\mathbf{g}_t\) is a vector of possible values that (gated by \(\mathbf{i}_t\)) can be added to \(\mathbf{c}_{t-1}\). The state vectors are then updated as follows:

\begin{align*}
  \mathbf{c}_t &= \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \mathbf{g}_t \\
  \mathbf{h}_t &= \mathbf{o}_t \odot \tanh(\mathbf{c}_t)
\end{align*}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{gru}
\caption{Architecture of a typical GRU cell. \(\mathbf{h}_{t-1}\) and the input \(\mathbf{x}_t\) are used to compute the reset gate \(\mathbf{r}_t\) and the update gate \(\mathbf{u}_t\). These gates are then used to compute a candidate vector \(\mathbf{h}_t'\) which in turn is then used to compute the new hidden state vector \(\mathbf{h}_t\).}
\label{gru}
\end{figure}

A more recent approach to optimizing RNNs are Gated Recurrent Units \cite{gru}. They have fewer parameters than LSTMs and work without an additional memory vector and are therefore faster to train and more light-weight (see Figure \ref{gru}). Recent research suggests that the performances of LSTM networks and of GRU networks are comparable \cite{lstm_vs_gru}.

In contrast to an LSTM, a GRU cell only has two gates, a reset gate \(\mathbf{r}_t\) and an update gate \(\mathbf{u}_t\). They are computed in a similar way as the gates of an LSTM:

\begin{align*}
  \mathbf{r}_t &= \sigma \left(\mathbf{W}_r \begin{pmatrix} \mathbf{x}_t \\ \mathbf{h}_{t-1} \end{pmatrix} \right) \\
  \mathbf{u}_t &= \sigma \left(\mathbf{W}_u \begin{pmatrix} \mathbf{x}_t \\ \mathbf{h}_{t-1} \end{pmatrix} \right)
\end{align*}

These gates are than used to calculate a candidate vector \(\mathbf{h}_t'\) of possible new values which is then used to compute the updated hidden state vector \(\mathbf{h}_t\).

\begin{align*}
  \mathbf{h}_t' &= \tanh \left(\mathbf{W}_{h'} \begin{pmatrix} \mathbf{x}_t \\ \mathbf{r}_t \odot \mathbf{h}_{t-1} \end{pmatrix} \right) \\
  \mathbf{h}_t &= (1 - \mathbf{u}_t) \odot \mathbf{h}_{t-1} + \mathbf{u}_t \odot \mathbf{h}_t'
\end{align*}

Almost all state of the art results today are achieved using either LSTMs, GRUs or similar networks with some form of gating mechanism because they are easier to train than vanilla RNNs and excel at capturing long-range dependencies.

\subsection{The Sequence-to-Sequence Model}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{seq2seq}
\caption{The sequence-to-sequence model applied to a translation example. The English source sentence is fed to the model word by word. After the input of an end-of-sequence token (\texttt{<eos>}), the network starts producing the output sentence in French. For this, the produced output tokens are fed back to the network at the next timestep. The network signals the end of the sequence by outputting another \texttt{<eos>} token.}
\label{seq2seq}
\end{figure}

Traditional deep neural networks process the whole input in one step and then calculate some output, e.g. process an image and then classify it. This works well for problems where the input and the output are of a fixed dimension, however, it is not suitable for problems where the input and the output are sequences of variable length. An example would be the input of a question and the network should produce an answer. We have seen that we can use LSTMs to process input sequences of variable length. However, in this case we want to process the whole input sequence and all the information that comes with it first and only then start generating an output sequence. These problems are called sequence to sequence problems.

In \cite{seq2seq} the Sequence-to-Sequence Model is introduced as a solution to these problems. The model was applied to the task of neural machine translation (NMT) and has since become the state of the art architecture in this field. The main concept can be seen in Figure \ref{seq2seq}. First, the whole input sequence is fed into the network and the output is ignored. Then an end-of-sequence token \texttt{<eos>} is input which signals the network to start producing the output. From there on the produced output tokens are fed to the network until another an end-of-sequence token is generated, thus signaling the end of the sequence. To speed up training the expected output is fed back to the network and not the actually produced output.

This architecture is further improved by splitting the network into two separate LSTMs. The first network takes all the input and encodes it into its hidden state vector which is then used to initialize the second network which is first fed a start token \texttt{<GO>} and then the generated output until the end of the sequence is reached.

These networks usually operate at word level and use some word embedding like word2vec \cite{word2vec}. This method has the advantage of giving the input words some meaning through the embedding instead of just inputting a meaningless encoding of the word. While this is very effective for translation tasks, there are some limitations to this method. Embeddings work on a fixed size vocabulary which means that out of vocabulary words (OOV) can't be handled. Also special character sequences like \texttt{:)} pose a problem.

\subsection{Attention-Mechanism}

Attention is a relatively new concept for neural networks. The idea is to allow the network to chose on which information to focus at any given moment. For example in \cite{visual_attention} attention is used on the task of high resolution image classification. These kinds of networks often struggle with memory constraints and attention can help them to only load the significant part of the image into the memory.

Attention has subsequently been applied to NMT \cite{attention_bahdanau,attention_luong}. The vector into which the input is encoded in the sequence-to-sequence model has been identified as a bottleneck which cuts down performance because of its limited capacity. After all the vector is of fixed dimensionality and needs to encode information about the whole input sequence. Because of that attention is used as a way for the decoder to peek at previous hidden states of the encoder. This is done with a context vector \(\tilde{\mathbf{c}}_t\) which is combined with the current hidden state of the decoder \(\mathbf{h}_t\). The resulting attentional hidden state \(\tilde{\mathbf{h}}_t\) is then used by the decoder to generate the next output.

\begin{equation*}
  \tilde{\mathbf{h}}_t = \tanh \left(\mathbf{W}_c \begin{pmatrix} \tilde{\mathbf{c}}_t \\ \mathbf{h}_{t} \end{pmatrix} \right)
\end{equation*}

For the derivation of the context vector \(\tilde{\mathbf{c}}_t\) all hidden states \(\bar{\mathbf{h}}_s\) of the encoder are considered. For this, an alignment vector \(\mathbf{a}_t\) whose size equals the input sequence length is derived from the current decoder hidden state \(\mathbf{h}_t\) and the encoder hidden states \(\bar{\mathbf{h}}_s\). The values of \(\mathbf{a}_t\) are normalized using the softmax function.

\begin{equation*}
  a_{ts} = \frac
            {\exp(\score(\mathbf{h}_t, \bar{\mathbf{h}}_s))}
            {\sum_{s'} \exp(\score(\mathbf{h}_t, \bar{\mathbf{h}}_{s'}))}
\end{equation*}

Here, \(\score\) is a content-based function used to compare the decoder hidden state \(\mathbf{h}_t\) with each one of the encoder hidden states \(\bar{\mathbf{h}}_s\). There are various possible choices for this function, for example:

\begin{equation*}
  \score(\mathbf{h}_t, \bar{\mathbf{h}}_s) =
  \begin{cases}
    \mathbf{h}_t^\intercal \mathbf{W}_a \bar{\mathbf{h}}_s \\
    \mathbf{v}_a^\intercal \tanh \left(\mathbf{W}_a \begin{pmatrix} \mathbf{h}_t \\ \bar{\mathbf{h}}_s \end{pmatrix} \right)
  \end{cases}
\end{equation*}

The context vector \(\tilde{\mathbf{c}}_t\) is then calculated as the weighted average over the encoder hidden states.

\begin{equation*}
  \tilde{\mathbf{c}}_t = \sum_{s'} a_{ts'} \bar{\mathbf{h}}_{s'}
\end{equation*}

\section{Model Implementation}

The implementation of the model used in this thesis is mostly based on the NMT model from Tensorflow \cite{seq2seq_tutorial}. The model consists of an encoder and a decoder with an implementation of the Luong attention mechanism \cite{attention_luong} on top. The encoder and the decoder are both an LSTM cell consisting of 4 layers \`a 256 units each. No embedding was used because the model operates at the character level instead of the word level and a single character doesn't have much meaning without its context. This change was necessary because programming languages don't have a fixed vocabulary. The programmer is not restricted in the naming of variables, methods or the like and thus it makes no sense to restrict the model to a fixed vocabulary. Otherwise, it would only result in a lot of OOV tokens. Therefore the input is fed one character at a time to the model with the encoding of the character simply being its ASCII code, i.e. a number between 0 and 127.

For inference, the decoder was first fed a start-of-sequence token and after that the produced output was fed back as an input to the decoder until an end-of-sequence token was output. During training, however, the correct target sequence was fed to the decoder, left padded by a start-of-sequence token to optimize training.

 During backpropagation, the gradients were clipped by a fixed norm. This technique is used to prevent the gradients from exploding \cite{gradient_clipping}. The norm chosen for this thesis is 5 but other values would also be possible (1 would be another common choice).

 The loss was measured with the cross-entropy loss function. Each timestep the decoder produces some output vector \(\mathbf{y}'^\intercal = \begin{pmatrix} y_1' & ... & y_n'\end{pmatrix}\). This vector is then normalized using the softmax function to get probabilities \(p_i\) for each possible output.

 \begin{equation*}
   p_i = \frac
             {\exp(y_i')}
             {\sum_{j} \exp(y_j')}
 \end{equation*}

 These probabilities are then used to compute the cross-entropy loss:

 \begin{equation*}
   l = - \sum_{j} y_j * \log(p_j)
 \end{equation*}

 Here \(\mathbf{y}^\intercal = \begin{pmatrix} y_1 & ... & y_n \end{pmatrix}\) is a one-hot target vector with \(y_i = 1\) for the desired output \(i\) and \(y_j = 0\) everywhere else.

 For each setting, training was done for 30,000 iterations while the batch size was set to 64. The max sequence length was set to 300 meaning that only examples of 300 characters or less were used to train and evaluate the model.

\section{Dataset Construction}

To train the model, a big dataset of erroneous code examples produced by real programmers including their respective corrections would be ideal. This would assure a large variety of errors and real-life examples. However, such a dataset does not exist in part because the correction of erroneous code is a long and tedious work which has to be done by hand.

The best alternative is to take a dataset of correct code and introduce the errors artificially. This task is no trivial one and several difficulties have to be taken into account and weighed up against each other. For one, the more sophisticated an error is the harder it is to introduce it consistently, but training a network on only easy errors (like missing semicolons) doesn't produce any added value. Another issue is the artificiality of the errors. One runs the risk of the model picking up on the error generation patterns and thus performing poorly on non-artificial examples. These problems are further discussed in Subsection \ref{corruption}.

For this thesis, the data from the Java Github Corpus \cite{java_dataset} was chosen. As elaborated in Section \ref{code_correction_section}, a weakly typed programming language like Python would be preferable over a strongly typed one like Java because a lot of errors in Java can already be found algorithmically. However, there is a general lack of large, diverse datasets of source code thus the selection of the Java Github Corpus. Furthermore, this thesis doesn't aim at building a fully polished "code corrector" but rather tries to test the boundaries of the possible. The model knows nothing of the structure and rules of the programming language and therefore the capability of the model to grasp certain concepts can still be tested.

The dataset was crawled from Github and includes only projects which were forked at least once to assure a certain measure of quality. It consists of around 15,000 projects which amount to approximately 15GB of data.

\subsection{Preprocessing}

Before the data was used, some preprocessing had to be done. While LSTMs work better than vanilla RNNs on long-range dependencies they still have their limits when it comes to input length \cite{timestep_limitation}. Because the input is fed to the network character-by-character rather than word-by-word, the input sequence can get quite long rather quickly. Therefore the decision was made, to concentrate on method declarations because they are relatively self-contained and complex enough to introduce advanced errors while also being of manageable length.

The preprocessing was done for each Java file in the dataset separately and consisted of the following steps:

\begin{enumerate}
  \item All comments were removed from the file because they are irrelevant for error detection and only increase the sequence length.
  \item Line breaks were replaced by an end-of-line token.
  \item All unnecessary whitespaces were removed. This was also done to reduce sequence length because Java is a whitespace insensitive programming language, i.e. a Java program is still valid (albeit harder to understand) if its indentation is removed.
  \item If the file still contained non-ASCII characters, it was discarded. The purpose of this was to get rid of very rare characters, to reduce the input and output space and most importantly to avoid encoding errors.
  \item All method declarations were extracted from the file and checked for their corruptibility. This means, that all corruptions (see Subsection \ref{corruption}) had to be able to be applied to the method. This way the rate at which the corruptions were introduced could be controlled better.
  \item All suitable methods were then written to new files (ca. 100MB each), one method per line.
\end{enumerate}

This resulted in around 1.7GB of train data.

\subsection{Corruptions}
\label{corruption}

As mentioned earlier, the generation of artificial errors is a challenging task. These errors need to be as sophisticated and as close to reality as possible else the learned model cannot be applied to real world examples. A large variety in the introduced errors would also be preferable. However, these guidelines are not easy to implement especially for more sophisticated errors. While it is quite easy to introduce syntax errors such as a missing semicolon, the task of automatically and unfailingly generating logic errors is very challenging. That's why this thesis concentrates on five different errors of variable difficulty.

The corruption of the data was done randomly during training. Each corruption was applied equally often, while the percentage of uncorrupted examples could be controlled. Of the possible corruptions, two produced syntax errors, two semantic errors and one logic errors. The syntax errors consisted of removing a bracket or a semicolon, for the semantic errors a variable was misspelled or the return type of the method changed and the logic errors were produced by switching the order of two statement lines. Examples and implementation details of the different corruptions can be found in Table \ref{corruption_table}.

Of the five possible corruptions, the syntax errors are the easiest to introduce and come relatively close to reality. They are typical errors that a novice programmer would produce and they are also the easiest ones to correct because the placement of semicolons and brackets follows strict rules. The other three corruptions produce more sophisticated errors but each one has some downsides.

The misspelling of a random variable works generally well. As a convention, the declaration of the variable is considered the "ground truth" and thus only occurrences of the variable after its declaration were misspelled. This is similar to how a traditional error checker would search for misspelled variables. However, there are some cases where the corruption doesn't work as intended. Consider the following code snippet:

\begin{lstlisting}[style=inline]
public int[] seedToArray(int seed){
  int[] seeds = new int[1];
  seeds[0] = seed;
  return seeds;
}
\end{lstlisting}

Let's assume that \texttt{seed} is to be corrupted. One possible misspelling would be to add a random character. If perchance an 's' is added to the end of the occurrence of \texttt{seed} in the third line, it produces another valid variable, namely \texttt{seeds}. Of course, the error could still be detected but it is now a more challenging one and the model probably doesn't catch up on this unless it encounters such errors more frequently. It is also possible, that the corruption switches two adjacent characters in \texttt{seed}. If the second and third characters are selected it would have no effect and therefore no error would be added. However, these two scenarios are very rare and therefore shouldn't impact the ability of the model to learn to find misspelled variables and correct them.

The other semantic error, the changing of the return type of the method, can be generated consistently but the generation sometimes imposes unsolvable problems to the network. The first case is pretty simple. If the return type of the method is changed from \texttt{void} to something different, the model only has to check if a value is returned and if not, the return type should be changed to \texttt{void}. The second case is more complicated as the return type is changed from a different type to \texttt{void}. Again the model can determine if the return type \texttt{void} is correct by looking if a value is returned. However, if it determines that it is incorrect the model still needs to derive the correct return type from the given context which is not always possible. For example, if the following source is given:

\begin{lstlisting}[style=inline]
public void incrementAndGetValue(){
  this.value += 1;
  return this.value;
}
\end{lstlisting}

\noindent the correct return type is not identifiable because \texttt{this.value} is not defined in the context of this method. It could be any numeric type. There is also the problem of the return type being a superclass or an interface of the returned object. This is the trade-off of only looking at methods as opposed to whole files. However, even with the full file context, the return type could still be defined in another file, for example, if the return type is determined by the return type of a method belonging to a different class. Despite this unsolvable problem the corruption can still be used to test the limits of what the model can learn.

Lastly, logic errors are the most challenging ones to automatically generate because they require some form of understanding of the source code. To keep the errors relatively realistic, while also keeping the corruption as simple and as accurate as possible, the switching of the order of two adjacent lines in the method was chosen. However, this corruption is not guaranteed to always produce a logic error and often times produces a semantic one instead. To further increase the probability of generating an error, some restrictions were put into place. Firstly, only variable declarations, assignments or method invocations were considered and secondly, only two adjacent lines which were of a different type could be switched. This increased the probability of the occurrence of an error, but it still didn't guarantee it. Consider the following example:

\begin{lstlisting}[style=inline]
public int squareSum(int a, int b){
  int squareA;
  int squareB;
  squareA = a * a;
  squareB = b * b;
  return squareA + squareB;
}
\end{lstlisting}

Here the only lines that can be switched according to the restrictions listed above are line 3 and 4 but no logic error is produced in doing so. However to produce logic errors more consistently a deeper understanding of the code would be necessary which is not possible if the errors are to be generated artificially. Having said that, the corruption with its restrictions was still deemed "good enough" to gain some insights into what the model is able to learn and what not.

\afterpage{
  \clearpage
  \pagestyle{empty}
  \begin{landscape}
    \begin{table}[t]
      \centering
      \begin{tabular}{ | m{4cm} m{2cm}  m{9cm}  m{5cm} | }
        \hline
        Corruption & Error Type & Explanation & Example \\
        \hline
        missing bracket &
        syntax &
        One random bracket (regular, curved or squared) is selected and removed from the source. &
        \lstinputlisting{listings/missing_bracket.java}
        \\
        missing semicolon &
        syntax &
        One random semicolon is selected and removed from the source. &
        \lstinputlisting{listings/missing_semicolon.java}
        \\
        misspelled variable &
        semantic &
        A random variable which is being declared in the source is selected. A random occurrence of the variable (except the one in the declaration) it then misspelled. Possible misspellings are the removal of a random character, the insertion of a random character or the switching of two adjacent characters. &
        \lstinputlisting{listings/misspelled_variable.java}
        \\
        incorrect return type &
        semantic &
        The return type of the method is changed. There are only two possibilities. If the return type is \texttt{void}, it is changed to one of Java's primitive data types (\texttt{int}, \texttt{float}, etc.). In any other case, the return type is changed to \texttt{void}. &
        \lstinputlisting{listings/incorrect_return_type.java}
        \\
        switched lines &
        semantic/ logic &
        Two adjacent statement lines are switched in their order. As statement lines qualify variable declarations, assignments and method invocations. To increase the probability of an error, only lines of different types can be switched. &
        \lstinputlisting{listings/switched_lines.java}
        \\
        \hline
      \end{tabular}
      \caption{Implementation details and examples of the used corruptions.}
      \label{corruption_table}
    \end{table}
  \end{landscape}
  \clearpage
}
