\noindent This thesis covers the implementation of an LSTM based sequence-to-sequence model with an attention mechanism on top as well as the difficulties of introducing errors into source code that have the requirement to be as close to reality and as sophisticated as possible. The model is then trained to detect and correct these artificially introduced syntax, semantic and logic errors. This thesis also provides experimental evaluations of several architectural design choices and a detailed analysis of the introduced errors. The model achieves promising results for all generated errors and manages to gain some understanding of the semantics of source code. However, it could not be conclusively determined if the model is also able to learn to detect and correct logic errors. 
