\section{Motivation}
Automatic text correction is a ubiquitous technology in our today world. Every smartphone, every word processing software, every browser provides some form of error detection and correction for text input. These systems rely on a vocabulary of correct words or some machine learning algorithm to find errors and possible corrections.

Obviously similar techniques can also be applied to source code. The syntax of a programming language is strictly defined which enables integrated development environments (IDEs) to detect syntax errors before the program is even run. Of course the IDE's possibilities are limited by the properties of the programming language, e.g. is it strongly typed or weakly typed. However it is impossible to work with a fixed vocabulary because the naming of variables, methods, etc. is not restricted to a particular spelling (except for keywords).

Furthermore, the error detection in source code is mostly limited to syntax errors, while semantic errors show only at runtime if at all. These errors are also the hardest ones to fix. In a strongly typed language like Java, a lot of possible errors in naming and accessing attributes can be eliminated, because each variable has to be initiated before it is used and the type of the variables is known at all times and therefore also their available attributes and methods. In weakly typed languages like Ruby however, one can not determine what type of object a variable holds before runtime. This creates additional sources of runtime errors.

Insert neural networks. Deep neural networks have proved to be very effective in such tasks that are infeasible to solve with traditional algorithms.

The aim of this project is it to train a character based sequence-to-sequence model on the task of source code correction. The implementation of the model is based on the neural machine translation (NMT) model provided by Tensorflow \cite{seq2seq_tutorial}. As a dataset the Java Github Corpus \cite{java_dataset} is used as a source of correct data. This data is then perturbed as random syntax and semantic errors are added. The performance of different model architectures is then evaluated for the introduced errors.

\section{Outline}
