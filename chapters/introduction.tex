\section{Motivation}
Automatic text correction is a ubiquitous technology in our today world. Every smartphone, every word processing software, every browser provides some form of spelling error detection for text input and these tools have proven to be very useful for both language learners and native speakers. These systems usually rely on a dictionary of correct words \cite{digram_correction, dictionary_correction} or some machine learning algorithm \cite{seq2seq_on_text_correction} to find errors and suggest possible corrections.

Source code is very sensitive to errors of all kinds and therefore a similar functionality is desirable for code editors. We distinguish between three types of errors in source code: syntax, semantic and logic errors (see Table \ref{error_table}). The syntax of a programming language is strictly defined which enables integrated development environments (IDEs) to detect syntax errors with the help of a parser or a suitable algorithm before the program is even run. However the capability of an IDE to detect more sophisticated errors is limited by the properties of the programming language, e.g. is it strongly typed or weakly typed. In general, the error detection in source code is mostly limited to syntax errors, while semantic and logic errors show only at runtime or go completely unnoticed. However, in a strongly typed language like Java, a lot of semantic errors can be detected because each variable has to be declared and initiated before it is used. The type of the variables is known at all times and therefore also their available attributes and methods. In weakly typed languages like Python on the other hand one can generally not determine what type of value a variable holds before runtime. This prevents an algorithm from detecting most semantic errors before runtime. Logic errors are in general impossible to find with a traditional algorithm because their detection requires knowledge of the purpose of the program and even humans struggle with this task. A model which is able to detect possible logic errors would therefore be extremely useful.

This is where machine learning algorithms could step in. In the past years, deep neural networks have proven to be very effective in learning generalized concepts and applying them to individual examples. For example in \cite{style_transfer} a network is trained to transfer the style of a painting to a video sequence. Neural networks can also be trained on the tasks of object instance segmentation in images \cite{segmentation} or face recognition \cite{face_recognition}. Another branch of research applies neural networks to natural language processing problems such as neural machine translation \cite{seq2seq}, keyboard text decoding \cite{seq2seq_keyboard} or automatic text correction \cite{seq2seq_on_text_correction}. These results suggest that it should also be possible to train a network on the detection and correction of more sophisticated errors in source code.

The goal of this thesis is it to train a character based sequence-to-sequence model \cite{seq2seq} on the task of source code correction. The model consists of an encoder network and a decoder network both of which are LSTMs \cite{lstm}. On top of that, an attention mechanism \cite{attention_luong} is used to allow the decoder to focus on important parts of the input sequence. Sequence-to-sequence is a technique originally proposed for the problem of neural machine translation but it can also be applied to any kind of problem where one sequence needs to be mapped to another sequence. The advantage of this architecture is that the model gets to see the whole input before producing any output which is necessary to correct certain errors in source code. By operating on the character level the network is also not limited to a fixed vocabulary of tokens which would be infeasible for source code.

 The implementation of the model is based on the neural machine translation model provided by Tensorflow \cite{seq2seq_tutorial} and the Java Github Corpus \cite{java_dataset} is used as a source of correct data. This data is then corrupted as random syntax, semantic and logic errors are added. A dataset of real-life examples would be preferable over these self-introduced errors but there simply is none. The difficulty for the automatic generation of these corruptions is that they have to be as sophisticated and as close to reality as possible which is not easy to do. Especially logic errors cannot be automatically generated without some knowledge of the purpose of the code. The approach selected in this thesis is to choose a corruption which has a high probability of generating logic errors and which can be introduced without some understanding of the purpose of the code.

\begin{table}[t]
\begin{tabular}{ | m{2cm}  m{35mm}  m{55mm} | }
  \hline
  Error Type & Definition & Example \\
  \hline
  syntax &
  Violation of the specified syntax of the programming language. &
  \begin{lstlisting}
  public int add(int a, int b){
    int sum := a + b;
    return sum;
  }
  \end{lstlisting}
  \\
  semantic &
  Incorrect usage of a variable or statement. &
  \begin{lstlisting}
  public void add(int a, int b){
    int sum = a + b;
    return sum;
  }
  \end{lstlisting}
  \\
  logic &
  Failure to comply with the programs requirements. &
  \begin{lstlisting}
  public int add(int a, int b){
    int sum = a - b;
    return sum;
  }
  \end{lstlisting}
  \\
  \hline
\end{tabular}
\caption{The definition of syntax, semantic and logic error in the context of programming languages.}
\label{error_table}
\end{table}

\section{Outline}

This thesis is divided into five chapters, including this introduction. In the second chapter, an overview of the existing work on the task of spell checking and spelling correction is given and also of some research on error detection and correction in source code. The third chapter starts with a short introduction to the machine learning techniques and architectures used in this thesis. It also provides a description of the used model, the training procedure, the dataset construction and the corruption of the input. In chapter four the experiments are explained and the obtained results analyzed. Chapter five provides the conclusion and suggestions for future work.
